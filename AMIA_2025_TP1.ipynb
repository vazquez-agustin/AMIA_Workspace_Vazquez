{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YoTVCUtTQL6c"
   },
   "source": [
    "# TP 1: LDA/QDA y optimización matemática de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kL_4etdeizy"
   },
   "source": [
    "# Intro teórica\n",
    "\n",
    "## Definición: Clasificador Bayesiano\n",
    "\n",
    "Sean $k$ poblaciones, $x \\in \\mathbb{R}^p$ puede pertenecer a cualquiera $g \\in \\mathcal{G}$ de ellas. Bajo un esquema bayesiano, se define entonces $\\pi_j \\doteq P(G = j)$ la probabilidad *a priori* de que $X$ pertenezca a la clase *j*, y se **asume conocida** la distribución condicional de cada observable dado su clase $f_j \\doteq f_{X|G=j}$.\n",
    "\n",
    "De esta manera dicha probabilidad *a posteriori* resulta\n",
    "$$\n",
    "P(G|_{X=x} = j) = \\frac{f_{X|G=j}(x) \\cdot p_G(j)}{f_X(x)} \\propto f_j(x) \\cdot \\pi_j\n",
    "$$\n",
    "\n",
    "La regla de decisión de Bayes es entonces\n",
    "$$\n",
    "H(x) \\doteq \\arg \\max_{g \\in \\mathcal{G}} \\{ P(G|_{X=x} = j) \\} = \\arg \\max_{g \\in \\mathcal{G}} \\{ f_j(x) \\cdot \\pi_j \\}\n",
    "$$\n",
    "\n",
    "es decir, se predice a $x$ como perteneciente a la población $j$ cuya probabilidad a posteriori es máxima.\n",
    "\n",
    "*Ojo, a no desesperar! $\\pi_j$ no es otra cosa que una constante prefijada, y $f_j$ es, en su esencia, un campo escalar de $x$ a simplemente evaluar.*\n",
    "\n",
    "## Distribución condicional\n",
    "\n",
    "Para los clasificadores de discriminante cuadrático y lineal (QDA/LDA) se asume que $X|_{G=j} \\sim \\mathcal{N}_p(\\mu_j, \\Sigma_j)$, es decir, se asume que cada población sigue una distribución normal.\n",
    "\n",
    "Por definición, se tiene entonces que para una clase $j$:\n",
    "$$\n",
    "f_j(x) = \\frac{1}{(2 \\pi)^\\frac{p}{2} \\cdot |\\Sigma_j|^\\frac{1}{2}} e^{- \\frac{1}{2}(x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j)}\n",
    "$$\n",
    "\n",
    "Aplicando logaritmo (que al ser una función estrictamente creciente no afecta el cálculo de máximos/mínimos), queda algo mucho más práctico de trabajar:\n",
    "\n",
    "$$\n",
    "\\log{f_j(x)} = -\\frac{1}{2}\\log |\\Sigma_j| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j) + C\n",
    "$$\n",
    "\n",
    "Observar que en este caso $C=-\\frac{p}{2} \\log(2\\pi)$, pero no se tiene en cuenta ya que al tener una constante aditiva en todas las clases, no afecta al cálculo del máximo.\n",
    "\n",
    "## LDA\n",
    "\n",
    "En el caso de LDA se hace una suposición extra, que es $X|_{G=j} \\sim \\mathcal{N}_p(\\mu_j, \\Sigma)$, es decir que las poblaciones no sólo siguen una distribución normal sino que son de igual matriz de covarianzas. Reemplazando arriba se obtiene entonces:\n",
    "\n",
    "$$\n",
    "\\log{f_j(x)} =  -\\frac{1}{2}\\log |\\Sigma| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j) + C\n",
    "$$\n",
    "\n",
    "Ahora, como $-\\frac{1}{2}\\log |\\Sigma|$ es común a todas las clases se puede incorporar a la constante aditiva y, distribuyendo y reagrupando términos sobre $(x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j)$ se obtiene finalmente:\n",
    "\n",
    "$$\n",
    "\\log{f_j(x)} =  \\mu_j^T \\Sigma^{-1} (x- \\frac{1}{2} \\mu_j) + C'\n",
    "$$\n",
    "\n",
    "## Entrenamiento/Ajuste\n",
    "\n",
    "Obsérvese que para ambos modelos, ajustarlos a los datos implica estimar los parámetros $(\\mu_j, \\Sigma_j) \\; \\forall j = 1, \\dots, k$ en el caso de QDA, y $(\\mu_j, \\Sigma)$ para LDA.\n",
    "\n",
    "Estos parámetros se estiman por máxima verosimilitud, de manera que los estimadores resultan:\n",
    "\n",
    "* $\\hat{\\mu}_j = \\bar{x}_j$ el promedio de los $x$ de la clase *j*\n",
    "* $\\hat{\\Sigma}_j = s^2_j$ la matriz de covarianzas estimada para cada clase *j*\n",
    "* $\\hat{\\pi}_j = f_{R_j} = \\frac{n_j}{n}$ la frecuencia relativa de la clase *j* en la muestra\n",
    "* $\\hat{\\Sigma} = \\frac{1}{n} \\sum_{j=1}^k n_j \\cdot s^2_j$ el promedio ponderado (por frecs. relativas) de las matrices de covarianzas de todas las clases. *Observar que se utiliza el estimador de MV y no el insesgado*\n",
    "\n",
    "Es importante notar que si bien todos los $\\mu, \\Sigma$ deben ser estimados, la distribución *a priori* puede no inferirse de los datos sino asumirse previamente, utilizándose como entrada del modelo.\n",
    "\n",
    "## Predicción\n",
    "\n",
    "Para estos modelos, al igual que para cualquier clasificador Bayesiano del tipo antes visto, la estimación de la clase es por método *plug-in* sobre la regla de decisión $H(x)$, es decir devolver la clase que maximiza $\\hat{f}_j(x) \\cdot \\hat{\\pi}_j$, o lo que es lo mismo $\\log\\hat{f}_j(x) + \\log\\hat{\\pi}_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IV8OF-SlPHbD"
   },
   "source": [
    "# Código provisto\n",
    "\n",
    "Con el fin de no retrasar al alumno con cuestiones estructurales y/o secundarias al tema que se pretende tratar, se provee una base de código que **no es obligatoria de usar** pero se asume que resulta resulta beneficiosa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PrDdJRypNB-y"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.linalg as LA\n",
    "from scipy.linalg import cholesky, solve_triangular\n",
    "from scipy.linalg.lapack import dtrtri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cPL33WIN2HA"
   },
   "source": [
    "## Base code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ewg5e0hsNTQC"
   },
   "outputs": [],
   "source": [
    "class BaseBayesianClassifier:\n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def _estimate_a_priori(self, y):\n",
    "    a_priori = np.bincount(y.flatten().astype(int)) / y.size\n",
    "    # Q3: para que sirve bincount?\n",
    "    return np.log(a_priori)\n",
    "\n",
    "  def _fit_params(self, X, y):\n",
    "    # estimate all needed parameters for given model\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
    "    # this should depend on the model used\n",
    "    raise NotImplementedError()\n",
    "\n",
    "  def fit(self, X, y, a_priori=None):\n",
    "    # if it's needed, estimate a priori probabilities\n",
    "    self.log_a_priori = self._estimate_a_priori(y) if a_priori is None else np.log(a_priori)\n",
    "\n",
    "    # now that everything else is in place, estimate all needed parameters for given model\n",
    "    self._fit_params(X, y)\n",
    "    # Q4: por que el _fit_params va al final? no se puede mover a, por ejemplo, antes de la priori?\n",
    "\n",
    "  def predict(self, X):\n",
    "    # this is actually an individual prediction encased in a for-loop\n",
    "    m_obs = X.shape[1]\n",
    "    y_hat = np.empty(m_obs, dtype=int)\n",
    "\n",
    "    for i in range(m_obs):\n",
    "      y_hat[i] = self._predict_one(X[:,i].reshape(-1,1))\n",
    "\n",
    "    # return prediction as a row vector (matching y)\n",
    "    return y_hat.reshape(1,-1)\n",
    "\n",
    "  def _predict_one(self, x):\n",
    "    # calculate all log posteriori probabilities (actually, +C)\n",
    "    log_posteriori = [ log_a_priori_i + self._predict_log_conditional(x, idx) for idx, log_a_priori_i\n",
    "                  in enumerate(self.log_a_priori) ]\n",
    "\n",
    "    # return the class that has maximum a posteriori probability\n",
    "    return np.argmax(log_posteriori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Rz2FC7A5NUpN"
   },
   "outputs": [],
   "source": [
    "class QDA(BaseBayesianClassifier):\n",
    "\n",
    "  def _fit_params(self, X, y):\n",
    "    # estimate each covariance matrix\n",
    "    self.inv_covs = [LA.inv(np.cov(X[:,y.flatten()==idx], bias=True))\n",
    "                      for idx in range(len(self.log_a_priori))]\n",
    "    # Q5: por que hace falta el flatten y no se puede directamente X[:,y==idx]?\n",
    "    # Q6: por que se usa bias=True en vez del default bias=False?\n",
    "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
    "                  for idx in range(len(self.log_a_priori))]\n",
    "    # Q7: que hace axis=1? por que no axis=0?\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
    "    # this should depend on the model used\n",
    "    inv_cov = self.inv_covs[class_idx]\n",
    "    unbiased_x =  x - self.means[class_idx]\n",
    "    return 0.5*np.log(LA.det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9lZbID0WNV1Y"
   },
   "outputs": [],
   "source": [
    "class TensorizedQDA(QDA):\n",
    "\n",
    "    def _fit_params(self, X, y):\n",
    "        # ask plain QDA to fit params\n",
    "        super()._fit_params(X,y)\n",
    "\n",
    "        # stack onto new dimension\n",
    "        self.tensor_inv_cov = np.stack(self.inv_covs)\n",
    "        self.tensor_means = np.stack(self.means)\n",
    "\n",
    "    def _predict_log_conditionals(self,x):\n",
    "        unbiased_x = x - self.tensor_means\n",
    "        inner_prod = unbiased_x.transpose(0,2,1) @ self.tensor_inv_cov @ unbiased_x\n",
    "\n",
    "        return 0.5*np.log(LA.det(self.tensor_inv_cov)) - 0.5 * inner_prod.flatten()\n",
    "\n",
    "    def _predict_one(self, x):\n",
    "        # return the class that has maximum a posteriori probability\n",
    "        return np.argmax(self.log_a_priori + self._predict_log_conditionals(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FasterQDA(TensorizedQDA):\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predice clases para múltiples observaciones en paralelo utilizando una versión\n",
    "        completamente vectorizada del modelo QDA.\n",
    "\n",
    "        Parámetros:\n",
    "        X : Matriz de observaciones (n muestras, cada una de dimensión p)\n",
    "\n",
    "        Retorna:\n",
    "        y_hat : Vector de predicciones, una por observación\n",
    "        \"\"\"\n",
    " \n",
    "        unbiased_X = X[None, :, :] - self.tensor_means # broadcasting automático resulta en un shape (k, p, n)\n",
    "        tmp = self.tensor_inv_cov @ unbiased_X # (k, p, p) @ (k, p, n) se transforma en (k, p, n)\n",
    "\n",
    "        # Se utiliza la distancia de Mahalanobis porque surge naturalmente del logaritmo de la densidad de la normal multivariada\n",
    "        #  y ajusta por escala y correlación entre variables.\n",
    "        mahalanobis_sq = np.sum(unbiased_X * tmp, axis=1) # shape (k, n)\n",
    "\n",
    "        # Determinantes de las matrices inversas de covarianza\n",
    "        log_det_inv = np.log(np.linalg.det(self.tensor_inv_cov))  # shape (k,)\n",
    "\n",
    "        # Log de la densidad condicional por clase y observación\n",
    "        log_conditionals = 0.5 * log_det_inv[:, None] - 0.5 * mahalanobis_sq  # shape (k, n)\n",
    "\n",
    "        # Se suman los log posteriores: log P(x|G=j) + log P(G=j)\n",
    "        log_posteriors = self.log_a_priori[:, None] + log_conditionals  # shape (k, n)\n",
    "\n",
    "        # Para cada observación se elege la clase con mayor log posterior\n",
    "        return np.argmax(log_posteriors, axis=0).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientQDA(TensorizedQDA):\n",
    "    \"\"\"\n",
    "    QDA eficiente que usa explícitamente la propiedad del punto 5:\n",
    "    diag(A·B) = sum(A ⊙ B^T, axis=1)\n",
    "    para evitar crear la matriz n×n intermedia.\n",
    "    \"\"\"\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predice clases usando la propiedad del punto 5 para evitar matriz n×n.\n",
    "        \n",
    "        Parámetros:\n",
    "        X : Matriz de observaciones (p, n) - p features, n muestras\n",
    "        \n",
    "        Retorna:\n",
    "        y_hat : Vector de predicciones (1, n)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Broadcasting: (k, p, 1) - (1, p, n) → (k, p, n)\n",
    "        unbiased_X = X[None, :, :] - self.tensor_means\n",
    "        \n",
    "        # Aplicamos Σ^{-1} a (X-μ): (k, p, p) @ (k, p, n) → (k, p, n)\n",
    "        inv_cov_times_unbiased = self.tensor_inv_cov @ unbiased_X\n",
    "        \n",
    "        # Propiedad del punto 5: sum((Σ^{-1}(X-μ)) ⊙ (X-μ), axis=1)\n",
    "        # Esto evita crear la matriz (k, n, n) intermedia\n",
    "        mahalanobis_sq = np.sum(inv_cov_times_unbiased * unbiased_X, axis=1)  # (k, n)\n",
    "        \n",
    "        # Determinantes de las matrices inversas de covarianza\n",
    "        log_det_inv = np.log(np.linalg.det(self.tensor_inv_cov))  # (k,)\n",
    "        \n",
    "        # Log de la densidad condicional por clase y observación\n",
    "        log_conditionals = 0.5 * log_det_inv[:, None] - 0.5 * mahalanobis_sq  # (k, n)\n",
    "        \n",
    "        # Log posteriores: log P(x|G=j) + log P(G=j)\n",
    "        log_posteriors = self.log_a_priori[:, None] + log_conditionals  # (k, n)\n",
    "        \n",
    "        # Clase con mayor log posterior para cada observación\n",
    "        return np.argmax(log_posteriors, axis=0).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "i-WGGi_sQ-pT"
   },
   "outputs": [],
   "source": [
    "class QDA_Chol1(BaseBayesianClassifier):\n",
    "  def _fit_params(self, X, y):\n",
    "    self.L_invs = [\n",
    "        LA.inv(cholesky(np.cov(X[:,y.flatten()==idx], bias=True), lower=True))\n",
    "        for idx in range(len(self.log_a_priori))\n",
    "    ]\n",
    "\n",
    "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
    "                  for idx in range(len(self.log_a_priori))]\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    L_inv = self.L_invs[class_idx]\n",
    "    unbiased_x =  x - self.means[class_idx]\n",
    "\n",
    "    y = L_inv @ unbiased_x\n",
    "\n",
    "    return np.log(L_inv.diagonal().prod()) -0.5 * (y**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "i5DNLtYbQsHi"
   },
   "outputs": [],
   "source": [
    "class QDA_Chol2(BaseBayesianClassifier):\n",
    "  def _fit_params(self, X, y):\n",
    "    self.Ls = [\n",
    "        cholesky(np.cov(X[:,y.flatten()==idx], bias=True), lower=True)\n",
    "        for idx in range(len(self.log_a_priori))\n",
    "    ]\n",
    "\n",
    "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
    "                  for idx in range(len(self.log_a_priori))]\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    L = self.Ls[class_idx]\n",
    "    unbiased_x =  x - self.means[class_idx]\n",
    "\n",
    "    y = solve_triangular(L, unbiased_x, lower=True)\n",
    "\n",
    "    return -np.log(L.diagonal().prod()) -0.5 * (y**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "v0dRvYVQRCgc"
   },
   "outputs": [],
   "source": [
    "class QDA_Chol3(BaseBayesianClassifier):\n",
    "  def _fit_params(self, X, y):\n",
    "    self.L_invs = [\n",
    "        dtrtri(cholesky(np.cov(X[:,y.flatten()==idx], bias=True), lower=True), lower=1)[0]\n",
    "        for idx in range(len(self.log_a_priori))\n",
    "    ]\n",
    "\n",
    "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True)\n",
    "                  for idx in range(len(self.log_a_priori))]\n",
    "\n",
    "  def _predict_log_conditional(self, x, class_idx):\n",
    "    L_inv = self.L_invs[class_idx]\n",
    "    unbiased_x =  x - self.means[class_idx]\n",
    "\n",
    "    y = L_inv @ unbiased_x\n",
    "\n",
    "    return np.log(L_inv.diagonal().prod()) -0.5 * (y**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCtrHQDuN6R4"
   },
   "source": [
    "## Datasets\n",
    "\n",
    "Observar que se proveen **4 datasets diferentes**, el código de ejemplo usa uno solo pero eso no significa que ustedes se limiten al mismo. También pueden usar otros datasets de su elección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "rasInBMFNzUH"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris, fetch_openml, load_wine\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_iris_dataset():\n",
    "  data = load_iris()\n",
    "  X_full = data.data\n",
    "  y_full = np.array([data.target_names[y] for y in data.target.reshape(-1,1)])\n",
    "  return X_full, y_full\n",
    "\n",
    "def get_penguins_dataset():\n",
    "    # get data\n",
    "    df, tgt = fetch_openml(name=\"penguins\", return_X_y=True, as_frame=True, parser='auto')\n",
    "\n",
    "    # drop non-numeric columns\n",
    "    df.drop(columns=[\"island\",\"sex\"], inplace=True)\n",
    "\n",
    "    # drop rows with missing values\n",
    "    mask = df.isna().sum(axis=1) == 0\n",
    "    df = df[mask]\n",
    "    tgt = tgt[mask]\n",
    "\n",
    "    return df.values, tgt.to_numpy().reshape(-1,1)\n",
    "\n",
    "def get_wine_dataset():\n",
    "    # get data\n",
    "    data = load_wine()\n",
    "    X_full = data.data\n",
    "    y_full = np.array([data.target_names[y] for y in data.target.reshape(-1,1)])\n",
    "    return X_full, y_full\n",
    "\n",
    "def get_letters_dataset():\n",
    "    # get data\n",
    "    letter = fetch_openml('letter', version=1, as_frame=False)\n",
    "    return letter.data, letter.target.reshape(-1,1)\n",
    "\n",
    "def label_encode(y_full):\n",
    "    return LabelEncoder().fit_transform(y_full.flatten()).reshape(y_full.shape)\n",
    "\n",
    "def split_transpose(X, y, test_size, random_state):\n",
    "    # X_train, X_test, y_train, y_test but all transposed\n",
    "    return [elem.T for elem in train_test_split(X, y, test_size=test_size, random_state=random_state)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybPkuBdDN42P"
   },
   "source": [
    "## Benchmarking\n",
    "\n",
    "Nota: esta clase fue creada bastante rápido y no pretende ser una plataforma súper confiable sobre la que basarse, sino más bien una herramienta simple con la que poder medir varios runs y agregar la información.\n",
    "\n",
    "En forma rápida, `warmup` es la cantidad de runs para warmup, `mem_runs` es la cantidad de runs en las que se mide el pico de uso de RAM y `n_runs` es la cantidad de runs en las que se miden tiempos.\n",
    "\n",
    "La razón por la que se separan es que medir memoria hace ~2.5x más lento cada run, pero al mismo tiempo se estabiliza mucho más rápido.\n",
    "\n",
    "**Importante:** tener en cuenta que los modelos que predicen en batch (usan `predict` directamente) deberían consumir, como mínimo, $n$ veces la memoria de los que predicen por observación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "nO4Py3CeNpKu"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "# from tqdm.notebook import tqdm\n",
    "from numpy.random import RandomState\n",
    "import tracemalloc\n",
    "\n",
    "RNG_SEED = 6553\n",
    "\n",
    "class Benchmark:\n",
    "    def __init__(self, X, y, n_runs=1000, warmup=100, mem_runs=100, test_sz=0.3, rng_seed=RNG_SEED, same_splits=True):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n = n_runs\n",
    "        self.warmup = warmup\n",
    "        self.mem_runs = mem_runs\n",
    "        self.test_sz = test_sz\n",
    "        self.det = same_splits\n",
    "        if self.det:\n",
    "            self.rng_seed = rng_seed\n",
    "        else:\n",
    "            self.rng = RandomState(rng_seed)\n",
    "\n",
    "        self.data = dict()\n",
    "\n",
    "        print(\"Benching params:\")\n",
    "        print(\"Total runs:\",self.warmup+self.mem_runs+self.n)\n",
    "        print(\"Warmup runs:\",self.warmup)\n",
    "        print(\"Peak Memory usage runs:\", self.mem_runs)\n",
    "        print(\"Running time runs:\", self.n)\n",
    "        approx_test_sz = int(self.y.size * self.test_sz)\n",
    "        print(\"Train size rows (approx):\",self.y.size - approx_test_sz)\n",
    "        print(\"Test size rows (approx):\",approx_test_sz)\n",
    "        print(\"Test size fraction:\",self.test_sz)\n",
    "\n",
    "    def bench(self, model_class, **kwargs):\n",
    "        name = model_class.__name__\n",
    "        time_data = np.empty((self.n, 3), dtype=float)  # train_time, test_time, accuracy\n",
    "        mem_data = np.empty((self.mem_runs, 2), dtype=float)  # train_peak_mem, test_peak_mem\n",
    "        rng = RandomState(self.rng_seed) if self.det else self.rng\n",
    "\n",
    "\n",
    "        for i in range(self.warmup):\n",
    "            # Instantiate model with error check for unsupported parameters\n",
    "            model = model_class(**kwargs)\n",
    "\n",
    "            # Generate current train-test split\n",
    "            X_train, X_test, y_train, y_test = split_transpose(\n",
    "                self.X, self.y,\n",
    "                test_size=self.test_sz,\n",
    "                random_state=rng\n",
    "            )\n",
    "            # Run training and prediction (timing or memory measurement not recorded)\n",
    "            model.fit(X_train, y_train)\n",
    "            model.predict(X_test)\n",
    "\n",
    "        for i in tqdm(range(self.mem_runs), total=self.mem_runs, desc=f\"{name} (MEM)\"):\n",
    "\n",
    "            model = model_class(**kwargs)\n",
    "\n",
    "            X_train, X_test, y_train, y_test = split_transpose(\n",
    "                self.X, self.y,\n",
    "                test_size=self.test_sz,\n",
    "                random_state=rng\n",
    "            )\n",
    "\n",
    "            tracemalloc.start()\n",
    "\n",
    "            t1 = time.perf_counter()\n",
    "            model.fit(X_train, y_train)\n",
    "            t2 = time.perf_counter()\n",
    "\n",
    "            _, train_peak = tracemalloc.get_traced_memory()\n",
    "            tracemalloc.reset_peak()\n",
    "\n",
    "            model.predict(X_test)\n",
    "            t3 = time.perf_counter()\n",
    "            _, test_peak = tracemalloc.get_traced_memory()\n",
    "            tracemalloc.stop()\n",
    "\n",
    "            mem_data[i,] = (\n",
    "                train_peak / (1024 * 1024),\n",
    "                test_peak / (1024 * 1024)\n",
    "            )\n",
    "\n",
    "        for i in tqdm(range(self.n), total=self.n, desc=f\"{name} (TIME)\"):\n",
    "            model = model_class(**kwargs)\n",
    "\n",
    "            X_train, X_test, y_train, y_test = split_transpose(\n",
    "                self.X, self.y,\n",
    "                test_size=self.test_sz,\n",
    "                random_state=rng\n",
    "            )\n",
    "\n",
    "            t1 = time.perf_counter()\n",
    "            model.fit(X_train, y_train)\n",
    "            t2 = time.perf_counter()\n",
    "            preds = model.predict(X_test)\n",
    "            t3 = time.perf_counter()\n",
    "\n",
    "            time_data[i,] = (\n",
    "                (t2 - t1) * 1000,\n",
    "                (t3 - t2) * 1000,\n",
    "                (y_test.flatten() == preds.flatten()).mean()\n",
    "            )\n",
    "\n",
    "        self.data[name] = (time_data, mem_data)\n",
    "\n",
    "    def summary(self, baseline=None):\n",
    "        aux = []\n",
    "        for name, (time_data, mem_data) in self.data.items():\n",
    "            result = {\n",
    "                'model': name,\n",
    "                'train_median_ms': np.median(time_data[:, 0]),\n",
    "                'train_std_ms': time_data[:, 0].std(),\n",
    "                'test_median_ms': np.median(time_data[:, 1]),\n",
    "                'test_std_ms': time_data[:, 1].std(),\n",
    "                'mean_accuracy': time_data[:, 2].mean(),\n",
    "                'train_mem_median_mb': np.median(mem_data[:, 0]),\n",
    "                'train_mem_std_mb': mem_data[:, 0].std(),\n",
    "                'test_mem_median_mb': np.median(mem_data[:, 1]),\n",
    "                'test_mem_std_mb': mem_data[:, 1].std()\n",
    "            }\n",
    "            aux.append(result)\n",
    "        df = pd.DataFrame(aux).set_index('model')\n",
    "\n",
    "        if baseline is not None and baseline in self.data:\n",
    "            df['train_speedup'] = df.loc[baseline, 'train_median_ms'] / df['train_median_ms']\n",
    "            df['test_speedup'] = df.loc[baseline, 'test_median_ms'] / df['test_median_ms']\n",
    "            df['train_mem_reduction'] = df.loc[baseline, 'train_mem_median_mb'] / df['train_mem_median_mb']\n",
    "            df['test_mem_reduction'] = df.loc[baseline, 'test_mem_median_mb'] / df['test_mem_median_mb']\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mb5VEpEugFXW"
   },
   "source": [
    "## Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fLyr4-hdgJ7e",
    "outputId": "86428138-3982-4c05-8a88-f6809d524a27"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((178, 13), (178, 1))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# levantamos el dataset Wine, que tiene 13 features y 178 observaciones en total\n",
    "X_full, y_full = get_wine_dataset()\n",
    "\n",
    "X_full.shape, y_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZxQlFUSbgYHQ",
    "outputId": "32c0fee8-2a79-4f8c-c526-392f026fff1e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([['class_0'],\n",
       "        ['class_0'],\n",
       "        ['class_0'],\n",
       "        ['class_0'],\n",
       "        ['class_0']], dtype='<U7'),\n",
       " array([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encodeamos a número las clases\n",
    "y_full_encoded = label_encode(y_full)\n",
    "\n",
    "y_full[:5], y_full_encoded[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pSBNNUOmgtsI",
    "outputId": "7dc60ba1-0548-4671-8f44-a9fb227360d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benching params:\n",
      "Total runs: 140\n",
      "Warmup runs: 20\n",
      "Peak Memory usage runs: 20\n",
      "Running time runs: 100\n",
      "Train size rows (approx): 125\n",
      "Test size rows (approx): 53\n",
      "Test size fraction: 0.3\n"
     ]
    }
   ],
   "source": [
    "# generamos el benchmark\n",
    "# observar que son valores muy bajos de runs para que corra rápido ahora\n",
    "b = Benchmark(\n",
    "    X_full, y_full_encoded,\n",
    "    n_runs = 100,\n",
    "    warmup = 20,\n",
    "    mem_runs = 20,\n",
    "    test_sz = 0.3,\n",
    "    same_splits = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "c01ec4015f1c4c4491687244977cf5ee",
      "f07c3a390d8640e586ec729d3c7c86d8",
      "1d3c4fad476c47208bce6625ec783f6d",
      "a33f440ec8454a2db5bd26485d71c4db",
      "766927c256e3409ab53c04ca092964e0",
      "54a06911978746dd9e81bb1526bd741c",
      "a806c90e849243999c2fce804e20b449",
      "0c4640fad3de46af8ee288fb94e9831d",
      "70c3a115637d4880802a605974661714",
      "3f110d0a165e41b0a268f77b2d2a11b7",
      "de398ad23ecd44d8b8d69e621401f3b3",
      "d0c5a2b6b27144268b7fc0bedcc33a86",
      "684de1d1bee34b309988bc0f40b0d5e2",
      "3a99fe4554914c12ad9146aea349708c",
      "1da7d96834cc4bc8a3cd5595b3446b74",
      "d21ef446c090431480050c3352e2634e",
      "d3fe060bd70848b7af130d99bbff6839",
      "32df4fc69a3c47cf9a220f4469b667a3",
      "254e2937d92d48b0886e5d950a6bbefa",
      "0cead8148ae5469eb637c29de2b21ec7",
      "de9f39ade1b04c22921589c687b25518",
      "437e4360cf5e44bc845c2f6002ea2d55"
     ]
    },
    "id": "zUciOjazhUu5",
    "outputId": "398afdb9-ef70-40db-b771-eb3f883a68f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QDA (MEM): 100%|██████████| 20/20 [00:00<00:00, 165.29it/s]\n",
      "QDA (TIME): 100%|██████████| 100/100 [00:00<00:00, 463.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# bencheamos un par\n",
    "to_bench = [QDA]\n",
    "\n",
    "for model in to_bench:\n",
    "    b.bench(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "8aeeac18744940f8b4940256613178db",
      "16f5ac0cf3cc441d891ae9522cf0798c",
      "bc84ff51ad3c4921b7aca214f0ff5e35",
      "f8508a02a66547b3a501694a1b8633ab",
      "4ab824715b294c86b045c7e187125524",
      "5e4f83990b0c4572952d5e23005283fb",
      "b85cd793196847cda92178f945624016",
      "b1c06f1940204bdaacf4ad3d843faa2f",
      "74c98c4598bd438cbedbb6c8a492eb15",
      "373a7a1549b44c2696083efddc88fafe",
      "8e2c2ff9e34b49568449aa7eb991d5cb",
      "de1229493a5749c799bcece8c07506f5",
      "c29e6bbacf9c45ecb5465624ee315b70",
      "a2e9076aca0b4224ba3aa8e4c3239260",
      "8df46d45d37e45e889dbec220fa360bd",
      "514491be27294e0cb5b70b6e25ee3355",
      "6d70e647d03f4004bd727673fbbfe5f9",
      "c3149d7e47c84328904326dda8527af4",
      "6197d697782b4688a278663e287317b1",
      "c69d75dc1d1b4fca925f308a79a7e248",
      "7f722aef90b84352b16d45f8716df168",
      "f356ef11b0734ce6a31ec9f0a01e055a"
     ]
    },
    "id": "wpPhSSCNhlvG",
    "outputId": "3edd56df-f71f-41bd-9f02-caa837541aa2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorizedQDA (MEM): 100%|██████████| 20/20 [00:00<00:00, 336.36it/s]\n",
      "TensorizedQDA (TIME): 100%|██████████| 100/100 [00:00<00:00, 674.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# como es una clase, podemos seguir bencheando más después\n",
    "b.bench(TensorizedQDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "id": "bZ5-vowshr5c",
    "outputId": "f494db5c-f2a5-46ba-a718-b7ba68f0699d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_median_ms</th>\n",
       "      <th>train_std_ms</th>\n",
       "      <th>test_median_ms</th>\n",
       "      <th>test_std_ms</th>\n",
       "      <th>mean_accuracy</th>\n",
       "      <th>train_mem_median_mb</th>\n",
       "      <th>train_mem_std_mb</th>\n",
       "      <th>test_mem_median_mb</th>\n",
       "      <th>test_mem_std_mb</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QDA</th>\n",
       "      <td>0.2175</td>\n",
       "      <td>0.045763</td>\n",
       "      <td>1.6595</td>\n",
       "      <td>0.306404</td>\n",
       "      <td>0.982407</td>\n",
       "      <td>0.018593</td>\n",
       "      <td>0.000642</td>\n",
       "      <td>0.007880</td>\n",
       "      <td>0.000042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TensorizedQDA</th>\n",
       "      <td>0.2817</td>\n",
       "      <td>0.149760</td>\n",
       "      <td>0.8493</td>\n",
       "      <td>0.174895</td>\n",
       "      <td>0.982593</td>\n",
       "      <td>0.018593</td>\n",
       "      <td>0.030797</td>\n",
       "      <td>0.012001</td>\n",
       "      <td>0.030742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train_median_ms  train_std_ms  test_median_ms  test_std_ms  \\\n",
       "model                                                                       \n",
       "QDA                     0.2175      0.045763          1.6595     0.306404   \n",
       "TensorizedQDA           0.2817      0.149760          0.8493     0.174895   \n",
       "\n",
       "               mean_accuracy  train_mem_median_mb  train_mem_std_mb  \\\n",
       "model                                                                 \n",
       "QDA                 0.982407             0.018593          0.000642   \n",
       "TensorizedQDA       0.982593             0.018593          0.030797   \n",
       "\n",
       "               test_mem_median_mb  test_mem_std_mb  \n",
       "model                                               \n",
       "QDA                      0.007880         0.000042  \n",
       "TensorizedQDA            0.012001         0.030742  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hacemos un summary\n",
    "b.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "09eKXqlXhwL-",
    "outputId": "a721b3e9-9003-4d2c-9f7e-11a96b0870d6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_median_ms</th>\n",
       "      <th>test_median_ms</th>\n",
       "      <th>mean_accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QDA</th>\n",
       "      <td>0.2175</td>\n",
       "      <td>1.6595</td>\n",
       "      <td>0.982407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TensorizedQDA</th>\n",
       "      <td>0.2817</td>\n",
       "      <td>0.8493</td>\n",
       "      <td>0.982593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train_median_ms  test_median_ms  mean_accuracy\n",
       "model                                                        \n",
       "QDA                     0.2175          1.6595       0.982407\n",
       "TensorizedQDA           0.2817          0.8493       0.982593"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# son muchos datos! nos quedamos con un par nomás\n",
    "summ = b.summary()\n",
    "\n",
    "# como es un pandas DataFrame, subseteamos columnas fácil\n",
    "summ[['train_median_ms', 'test_median_ms','mean_accuracy']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "id": "EopB9574h8I5",
    "outputId": "b8bf48a0-f791-4a34-da7c-254071cbadf7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_median_ms</th>\n",
       "      <th>train_std_ms</th>\n",
       "      <th>test_median_ms</th>\n",
       "      <th>test_std_ms</th>\n",
       "      <th>mean_accuracy</th>\n",
       "      <th>train_mem_median_mb</th>\n",
       "      <th>train_mem_std_mb</th>\n",
       "      <th>test_mem_median_mb</th>\n",
       "      <th>test_mem_std_mb</th>\n",
       "      <th>train_speedup</th>\n",
       "      <th>test_speedup</th>\n",
       "      <th>train_mem_reduction</th>\n",
       "      <th>test_mem_reduction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QDA</th>\n",
       "      <td>0.2175</td>\n",
       "      <td>0.045763</td>\n",
       "      <td>1.6595</td>\n",
       "      <td>0.306404</td>\n",
       "      <td>0.982407</td>\n",
       "      <td>0.018593</td>\n",
       "      <td>0.000642</td>\n",
       "      <td>0.007880</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TensorizedQDA</th>\n",
       "      <td>0.2817</td>\n",
       "      <td>0.149760</td>\n",
       "      <td>0.8493</td>\n",
       "      <td>0.174895</td>\n",
       "      <td>0.982593</td>\n",
       "      <td>0.018593</td>\n",
       "      <td>0.030797</td>\n",
       "      <td>0.012001</td>\n",
       "      <td>0.030742</td>\n",
       "      <td>0.772098</td>\n",
       "      <td>1.953962</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.656627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train_median_ms  train_std_ms  test_median_ms  test_std_ms  \\\n",
       "model                                                                       \n",
       "QDA                     0.2175      0.045763          1.6595     0.306404   \n",
       "TensorizedQDA           0.2817      0.149760          0.8493     0.174895   \n",
       "\n",
       "               mean_accuracy  train_mem_median_mb  train_mem_std_mb  \\\n",
       "model                                                                 \n",
       "QDA                 0.982407             0.018593          0.000642   \n",
       "TensorizedQDA       0.982593             0.018593          0.030797   \n",
       "\n",
       "               test_mem_median_mb  test_mem_std_mb  train_speedup  \\\n",
       "model                                                               \n",
       "QDA                      0.007880         0.000042       1.000000   \n",
       "TensorizedQDA            0.012001         0.030742       0.772098   \n",
       "\n",
       "               test_speedup  train_mem_reduction  test_mem_reduction  \n",
       "model                                                                 \n",
       "QDA                1.000000                  1.0            1.000000  \n",
       "TensorizedQDA      1.953962                  1.0            0.656627  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# podemos setear un baseline para que fabrique columnas de comparación\n",
    "summ = b.summary(baseline='QDA')\n",
    "\n",
    "summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "z0qeE1gviFLZ",
    "outputId": "760de925-f8fc-4e77-a8ce-50785d14e487"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_median_ms</th>\n",
       "      <th>test_median_ms</th>\n",
       "      <th>mean_accuracy</th>\n",
       "      <th>train_speedup</th>\n",
       "      <th>test_speedup</th>\n",
       "      <th>train_mem_reduction</th>\n",
       "      <th>test_mem_reduction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>QDA</th>\n",
       "      <td>0.2175</td>\n",
       "      <td>1.6595</td>\n",
       "      <td>0.982407</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TensorizedQDA</th>\n",
       "      <td>0.2817</td>\n",
       "      <td>0.8493</td>\n",
       "      <td>0.982593</td>\n",
       "      <td>0.772098</td>\n",
       "      <td>1.953962</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.656627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train_median_ms  test_median_ms  mean_accuracy  train_speedup  \\\n",
       "model                                                                          \n",
       "QDA                     0.2175          1.6595       0.982407       1.000000   \n",
       "TensorizedQDA           0.2817          0.8493       0.982593       0.772098   \n",
       "\n",
       "               test_speedup  train_mem_reduction  test_mem_reduction  \n",
       "model                                                                 \n",
       "QDA                1.000000                  1.0            1.000000  \n",
       "TensorizedQDA      1.953962                  1.0            0.656627  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summ[[\n",
    "    'train_median_ms', 'test_median_ms','mean_accuracy',\n",
    "    'train_speedup', 'test_speedup',\n",
    "    'train_mem_reduction', 'test_mem_reduction'\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EF80Pck2RmaC"
   },
   "source": [
    "# Consigna QDA\n",
    "\n",
    "**Notación**: en general notamos\n",
    "\n",
    "* $k$ la cantidad de clases\n",
    "* $n$ la cantidad de observaciones\n",
    "* $p$ la cantidad de features/variables/predictores\n",
    "\n",
    "**Sugerencia:** combinaciones adecuadas de `transpose`, `stack`, `reshape` y, ocasionalmente, `flatten` y `diagonal` suele ser más que suficiente. Se recomienda *fuertemente* explorar la dimensionalidad de cada elemento antes de implementar las clases.\n",
    "\n",
    "## Tensorización\n",
    "\n",
    "En esta sección nos vamos a ocupar de hacer que el modelo sea más rápido para generar predicciones, observando que incurre en un doble `for` dado que predice en forma individual un escalar para cada observación, para cada clase. Paralelizar ambos vía tensorización suena como una gran vía de mejora de tiempos.\n",
    "\n",
    "### 1) Diferencias entre `QDA`y `TensorizedQDA`\n",
    "\n",
    "1. ¿Sobre qué paraleliza `TensorizedQDA`? ¿Sobre las $k$ clases, las $n$ observaciones a predecir, o ambas?\n",
    "2. Analizar los shapes de `tensor_inv_covs` y `tensor_means` y explicar paso a paso cómo es que `TensorizedQDA` llega a predecir lo mismo que `QDA`.\n",
    "\n",
    "### 2) Optimización\n",
    "\n",
    "Debido a la forma cuadrática de QDA, no se puede predecir para $n$ observaciones en una sola pasada (utilizar $X \\in \\mathbb{R}^{p \\times n}$ en vez de $x \\in \\mathbb{R}^p$) sin pasar por una matriz de $n \\times n$ en donde se computan todas las interacciones entre observaciones. Se puede acceder al resultado recuperando sólo la diagonal de dicha matriz, pero resulta ineficiente en tiempo y (especialmente) en memoria. Aún así, es *posible* que el modelo funcione más rápido.\n",
    "\n",
    "3. Implementar el modelo `FasterQDA` (se recomienda heredarlo de `TensorizedQDA`) de manera de eliminar el ciclo for en el método predict.\n",
    "4. Mostrar dónde aparece la mencionada matriz de $n \\times n$, donde $n$ es la cantidad de observaciones a predecir.\n",
    "5. Demostrar que\n",
    "$$\n",
    "diag(A \\cdot B) = \\sum_{cols} A \\odot B^T = np.sum(A \\odot B^T, axis=1)\n",
    "$$ es decir, que se puede \"esquivar\" la matriz de $n \\times n$ usando matrices de $n \\times p$. También se puede usar, de forma equivalente,\n",
    "$$\n",
    "np.sum(A^T \\odot B, axis=0).T\n",
    "$$\n",
    "queda a preferencia del alumno cuál usar.\n",
    "6. Utilizar la propiedad antes demostrada para reimplementar la predicción del modelo `FasterQDA` de forma eficiente en un nuevo modelo `EfficientQDA`.\n",
    "7. Comparar la performance de las 4 variantes de QDA implementadas hasta ahora (no Cholesky) ¿Qué se observa? A modo de opinión ¿Se condice con lo esperado?\n",
    "\n",
    "## Cholesky\n",
    "\n",
    "Hasta ahora todos los esfuerzos fueron enfocados en realizar una predicción más rápida. Los tiempos de entrenamiento (teóricos al menos) siguen siendo los mismos o hasta (minúsculamente) peores, dado que todas las mejoras siguen llamando al método `_fit_params` original de `QDA`.\n",
    "\n",
    "La descomposición/factorización de [Cholesky](https://en.wikipedia.org/wiki/Cholesky_decomposition#Statement) permite factorizar una matriz definida positiva $A = LL^T$ donde $L$ es una matriz triangular inferior. En particular, si bien se asume que $p \\ll n$, invertir la matriz de covarianzas $\\Sigma$ para cada clase impone un cuello de botella que podría alivianarse. Teniendo en cuenta que las matrices de covarianza son simétricas y salvo degeneración, definidas positivas, Cholesky como mínimo debería permitir invertir la matriz más rápido.\n",
    "\n",
    "*Nota: observar que calcular* $A^{-1}b$ *equivale a resolver el sistema* $Ax=b$.\n",
    "\n",
    "### 3) Diferencias entre implementaciones de `QDA_Chol`\n",
    "\n",
    "8. Si una matriz $A$ tiene fact. de Cholesky $A=LL^T$, expresar $A^{-1}$ en términos de $L$. ¿Cómo podría esto ser útil en la forma cuadrática de QDA?\n",
    "7. Explicar las diferencias entre `QDA_Chol1`y `QDA` y cómo `QDA_Chol1` llega, paso a paso, hasta las predicciones.\n",
    "8. ¿Cuáles son las diferencias entre `QDA_Chol1`, `QDA_Chol2` y `QDA_Chol3`?\n",
    "9. Comparar la performance de las 7 variantes de QDA implementadas hasta ahora ¿Qué se observa?¿Hay alguna de las implementaciones de `QDA_Chol` que sea claramente mejor que las demás?¿Alguna que sea peor?\n",
    "\n",
    "### 4) Optimización\n",
    "\n",
    "12. Implementar el modelo `TensorizedChol` paralelizando sobre clases/observaciones según corresponda. Se recomienda heredarlo de alguna de las implementaciones de `QDA_Chol`, aunque la elección de cuál de ellas queda a cargo del alumno según lo observado en los benchmarks de puntos anteriores.\n",
    "13. Implementar el modelo `EfficientChol` combinando los insights de `EfficientQDA` y `TensorizedChol`. Si se desea, se puede implementar `FasterChol` como ayuda, pero no se contempla para el punto.\n",
    "13. Comparar la performance de las 9 variantes de QDA implementadas ¿Qué se observa? A modo de opinión ¿Se condice con lo esperado?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qcqVvRSLwaEZ"
   },
   "source": [
    "## Importante:\n",
    "\n",
    "Las métricas que se observan al realizar benchmarking son muy dependientes del código que se ejecuta, y por tanto de las versiones de las librerías utilizadas. Una forma de unificar esto es utilizando un gestor de versiones y paquetes como _uv_ o _Poetry_, otra es simplemente usando una misma VM como la que provee Colab.\n",
    "\n",
    "**Cada equipo debe informar las versiones de Python, NumPy y SciPy con que fueron obtenidos los resultados. En caso de que sean múltiples, agregar todos los casos**. La siguiente celda provee una ayuda para hacerlo desde un notebook, aunque como es una secuencia de comandos también sirve para consola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RwlW7sqwirdn",
    "outputId": "ccccb229-089a-44fb-cda3-da0a11b4fd4f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<3>WSL (9 - Relay) ERROR: CreateProcessCommon:640: execvpe(/bin/bash) failed: No such file or directory\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'python --version\\npip freeze | grep -E \"scipy|numpy\"\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCalledProcessError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m get_ipython().run_cell_magic(\u001b[33m'\u001b[39m\u001b[33mbash\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mpython --version\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mpip freeze | grep -E \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mscipy|numpy\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ia-env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2547\u001b[39m, in \u001b[36mInteractiveShell.run_cell_magic\u001b[39m\u001b[34m(self, magic_name, line, cell)\u001b[39m\n\u001b[32m   2545\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m   2546\u001b[39m     args = (magic_arg_s, cell)\n\u001b[32m-> \u001b[39m\u001b[32m2547\u001b[39m     result = fn(*args, **kwargs)\n\u001b[32m   2549\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2550\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2551\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2552\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ia-env\\Lib\\site-packages\\IPython\\core\\magics\\script.py:159\u001b[39m, in \u001b[36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[39m\u001b[34m(line, cell)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    158\u001b[39m     line = script\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.shebang(line, cell)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ia-env\\Lib\\site-packages\\IPython\\core\\magics\\script.py:336\u001b[39m, in \u001b[36mScriptMagics.shebang\u001b[39m\u001b[34m(self, line, cell)\u001b[39m\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args.raise_error \u001b[38;5;129;01mand\u001b[39;00m p.returncode != \u001b[32m0\u001b[39m:\n\u001b[32m    332\u001b[39m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[32m    333\u001b[39m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[32m    334\u001b[39m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[32m    335\u001b[39m     rc = p.returncode \u001b[38;5;129;01mor\u001b[39;00m -\u001b[32m9\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[31mCalledProcessError\u001b[39m: Command 'b'python --version\\npip freeze | grep -E \"scipy|numpy\"\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python --version\n",
    "pip freeze | grep -E \"scipy|numpy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNWRw6ofUNgZ"
   },
   "source": [
    "**Comentario:** yo utilicé los siguientes parámetros para mi run de prueba. Esto NO significa que ustedes tengan que usar los mismos, tampoco el mismo dataset. Se agregó al notebook simplemente porque fue una pregunta común en cohortes anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Vn-q4RJv8aA"
   },
   "outputs": [],
   "source": [
    "# dataset de letters\n",
    "X_letter, y_letter = get_letters_dataset()\n",
    "\n",
    "# encoding de labels\n",
    "y_letter_encoded = label_encode(y_letter.reshape(-1,1))\n",
    "\n",
    "# instanciacion del benchmark\n",
    "b = Benchmark(\n",
    "    X_letter, y_letter_encoded,\n",
    "    same_splits=False,\n",
    "    n_runs=100,\n",
    "    warmup=20,\n",
    "    mem_runs=30,\n",
    "    test_sz=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Respuestas:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enunciado N°1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TensorizedQDA` implementa una version  vectorizada (o tensorizada) del clasificador QDA parelizando así simultáneamente las k clases y las n observaciones a predecir.\n",
    "En contraste `QDA` predice la clase para cada observación una a una y para cada clase hace un cálculo individual. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enunciado N°2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las variables `tensor_inv_covs` y `tensor_means` estan definidas en `TensorizedQDA` como sigue:\n",
    "* `self.tensor_inv_cov = np.stack(self.inv_covs)` donde `self.inv_covs` es la lista de matrices de covarianza inversa $\\Sigma_j^{-1}$ y tiene tamaño $p \\times p$.\n",
    "* `self.tensor_means = np.stack(self.means)` donde `self.means` es la lista de $k$ vectores $\\mu_j$, cada uno de tamaño $p \\times 1$.\n",
    "\n",
    "Lo anterior se va a convertir en tensores con los siguientes shapes:\n",
    "\n",
    "* `self.tensor_inv_cov` tiene el shape $(k, p, p)$ luego de apilar `self.inv_covs` con `np.stack`\n",
    "* `self.tensor_means` tiene el shape $(k, p, 1)$ luego de apilar `self.means` con `np.stack`\n",
    "\n",
    "Para analizar como `TensorizedQDA` llega al mismo resultado que `QDA` podemos fijarnos lo siguiente en el método `TensorizedQDA._predict_log_conditionals(self, x)`:\n",
    "* `x` tiene shape $(p, n)$  \n",
    "* Para restar con broadcasting:  \n",
    "  $$\n",
    "  \\texttt{unbiased\\_x} = x - \\texttt{self.tensor\\_means} \\quad \\Rightarrow \\quad \\texttt{unbiased\\_x.shape} = (k, p, n)\n",
    "  $$\n",
    "  Cada media de clase $k$ se resta a las $n$ observaciones.\n",
    "  * Entonces el álculo del producto interno:\n",
    "  $$\n",
    "  \\texttt{inner\\_prod} = \\texttt{unbiased\\_x.transpose(0,2,1)} \\ @ \\ \\texttt{self.tensor\\_inv\\_cov} \\ @ \\ \\texttt{unbiased\\_x}\n",
    "  $$\n",
    "  - $\\texttt{unbiased\\_x.transpose(0,2,1)}$: cambia a shape $(k, n, p)$  \n",
    "  - Multiplicaciones:\n",
    "    $$\n",
    "    (k, n, p) \\ @ \\ (k, p, p) \\ \\Rightarrow \\ (k, n, p) \\\\\n",
    "    (k, n, p) \\ @ \\ (k, p, n) \\ \\Rightarrow \\ (k, n, n)\n",
    "    $$\n",
    "\n",
    "  Este resultado anterior corresponde:\n",
    "  $$\n",
    "  (x - \\mu_j)^T \\Sigma_j^{-1} (x - \\mu_j)\n",
    "  $$\n",
    "\n",
    "- Finalmente, se evalúa la densidad logarítmica:\n",
    "  $$\n",
    "  \\texttt{return} \\quad 0.5 \\cdot \\log \\det(\\Sigma_j^{-1}) - 0.5 \\cdot \\texttt{inner\\_prod.flatten()}\n",
    "  $$\n",
    "\n",
    "- `LA.det(self.tensor_inv_cov)` devuelve un vector de determinantes por clase $(k,)$  \n",
    "\n",
    "El resultado final es un vector con el logaritmo condicional para cada clase y observación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enunciado N°3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La implementación de `FasterQDA` elimina el ciclo `for` en el método `predict` gracias a la vectorización completa de las operaciones, usando tensores de dimensiones `(k, p, n)` para calcular simultáneamente las distancias de Mahalanobis y log-verosimilitudes para todas las clases y observaciones. Esto mejora significativamente la eficiencia computacional al aprovechar broadcasting y multiplicaciones matriciales en bloque (Ver la implementación de `FasterQDA`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enunciado N°4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando se calcula la cantidad $ (X - \\mu)^T A (X - \\mu) $ la cual está implementada en la línea de código:\n",
    "\n",
    "`inner_prod = unbiased_x.transpose(0,2,1) @ self.tensor_inv_cov @ unbiased_x` de `TensorizedQDA`\n",
    "\n",
    "donde:\n",
    "\n",
    "* $X \\in \\mathbb{R}^{p \\times n}$ es el conjunto de observaciones,\n",
    "\n",
    "* $\\mu \\in \\mathbb{R}^{p \\times 1}$ es el vector media,\n",
    "\n",
    "* $A = \\Sigma^{-1} \\in \\mathbb{R}^{p \\times p}$ es la matriz inversa de covarianza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enunciado N°5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demostrar la identidad para `diag(AB)`\n",
    "\n",
    "**Enunciado.**  \n",
    "Sean $A \\in \\mathbb{R}^{n \\times p}$ y $B \\in \\mathbb{R}^{p \\times n}$.  \n",
    "Queremos demostrar que:\n",
    "$$\n",
    "\\boxed{\\operatorname{diag}(A B)=\\sum_{\\text{cols}}(A\\odot B^{\\top})\n",
    "=\\mathrm{np.sum}(A\\odot B^{\\top},\\ \\text{axis}=1)}\n",
    "$$\n",
    "y de forma equivalente:\n",
    "$$\n",
    "\\boxed{\\operatorname{diag}(A B)=\\Big(\\mathrm{np.sum}(A^{\\top}\\odot B,\\ \\text{axis}=0)\\Big)^{\\!\\top}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Demostración paso a paso\n",
    "\n",
    "1. La componente \\( i \\)-ésima del vector ${diag}(AB)$ es:\n",
    "   $$\n",
    "   [\\operatorname{diag}(AB)]_i = (AB)_{ii}.\n",
    "   $$\n",
    "\n",
    "2. Por definición del producto matricial:\n",
    "   $$\n",
    "   (AB)_{ii} = \\sum_{j=1}^{p} A_{ij} B_{ji}.\n",
    "   $$\n",
    "\n",
    "3. Como $(B^{\\top})_{ij} = B_{ji}$, tenemos:\n",
    "   $$\n",
    "   (AB)_{ii} = \\sum_{j=1}^{p} A_{ij} (B^\\top)_{ij}.\n",
    "   $$\n",
    "\n",
    "4. Esta expresión es exactamente la **suma por columnas** del producto de Hadamard $A \\odot B^\\top$ para la fila $i$:\n",
    "   $$\n",
    "   \\operatorname{diag}(AB) = \\sum_{\\text{cols}} (A \\odot B^\\top) \n",
    "   = \\mathrm{np.sum}(A \\odot B^\\top,\\ \\text{axis}=1).\n",
    "   $$\n",
    "\n",
    "5. Alternativamente, si sumamos por filas en $A^\\top \\odot B$ y luego transponemos, obtenemos:\n",
    "   $$\n",
    "   \\operatorname{diag}(AB)=\\Big(\\mathrm{np.sum}(A^{\\top}\\odot B,\\ \\text{axis}=0)\\Big)^{\\!\\top}.\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enunciado N°6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EfficientQDA (evitar la matriz $n \\times n$)\n",
    "\n",
    "En QDA, para cada clase $k$:\n",
    "\n",
    "$$\n",
    "\\delta_k(x) = -\\tfrac{1}{2} \\log |\\Sigma_k| - \\tfrac{1}{2} (x - \\mu_k)^\\top \\Sigma_k^{-1} (x - \\mu_k) + \\log \\pi_k.\n",
    "$$\n",
    "\n",
    "En batch, con $X \\in \\mathbb{R}^{n \\times d}$ y $D_k = X - \\mu_k$ (broadcast), la parte cuadrática es\n",
    "$$\n",
    "\\operatorname{diag}\\bigl(D_k \\ \\Sigma_k^{-1} \\ D_k^\\top \\bigr).\n",
    "$$\n",
    "\n",
    "Aplicando (5) con $A = D_k \\Sigma_k^{-1}$ y $B = D_k^\\top$:\n",
    "\n",
    "$$\n",
    "\\operatorname{diag}(D_k \\Sigma_k^{-1} D_k^\\top) \n",
    "= \\sum_{\\text{cols}} \\bigl((D_k \\Sigma_k^{-1}) \\odot D_k\\bigr)\n",
    "= \\mathrm{np.sum}((D_k @ \\Sigma_k^{-1}) * D_k,\\ \\text{axis}=1).\n",
    "$$\n",
    "\n",
    "Esto evita construir la matriz $n \\times n$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def qda_scores_efficient(X, mus, Sigmas_inv, logdets, log_priors):\n",
    "    \"\"\"\n",
    "    X: (n,d)\n",
    "    mus: list/array of K means, each (d,)\n",
    "    Sigmas_inv: list/array of K inversas de covarianzas, cada una (d,d)\n",
    "    logdets: array shape (K,) con log|Sigma_k|\n",
    "    log_priors: array shape (K,) con log pi_k\n",
    "    return: scores (n,K)\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    K = len(mus)\n",
    "    scores = np.empty((n, K), dtype=float)\n",
    "\n",
    "    for k in range(K):\n",
    "        mu = mus[k]               # (d,)\n",
    "        S_inv = Sigmas_inv[k]     # (d,d)\n",
    "        D = X - mu                # (n,d)\n",
    "        # parte cuadrática por identidad del punto (5)\n",
    "        quad = np.sum((D @ S_inv) * D, axis=1)  # (n,)\n",
    "        scores[:, k] = -0.5 * (quad + logdets[k]) + log_priors[k]\n",
    "\n",
    "    return scores\n",
    "\n",
    "def qda_predict_efficient(X, mus, Sigmas_inv, logdets, log_priors):\n",
    "    scores = qda_scores_efficient(X, mus, Sigmas_inv, logdets, log_priors)\n",
    "    y_hat = np.argmax(scores, axis=1)\n",
    "    return y_hat, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enunciado N°7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive loop              0.330 s\n",
      "Vector + diag(nxn)      7.256 s\n",
      "FasterQDA               0.030 s\n",
      "EfficientQDA            0.026 s\n",
      "✓ Todas las variantes dan el mismo resultado.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from time import perf_counter\n",
    "\n",
    "def make_synth(n=20000, d=20, K=3, seed=0):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    X = rng.normal(size=(n,d))\n",
    "    mus = [rng.normal(size=d) for _ in range(K)]\n",
    "    Sigmas = []\n",
    "    Sigmas_inv = []\n",
    "    logdets = np.empty(K)\n",
    "    for k in range(K):\n",
    "        A = rng.normal(size=(d,d))\n",
    "        S = A @ A.T + 1e-3*np.eye(d)  # SPD\n",
    "        Sigmas.append(S)\n",
    "        Sigmas_inv.append(np.linalg.inv(S))\n",
    "        logdets[k] = np.log(np.linalg.det(S))\n",
    "    pis = np.ones(K)/K\n",
    "    log_priors = np.log(pis)\n",
    "    return X, mus, Sigmas, Sigmas_inv, logdets, log_priors\n",
    "\n",
    "# Aquí se conectan las 4 implementaciones:\n",
    "def qda_scores_naive_loop(X, mus, Sigmas_inv, logdets, log_priors):\n",
    "    # EJEMPLO simple y lento solo para test de igualdad:\n",
    "    n, d = X.shape\n",
    "    K = len(mus)\n",
    "    out = np.empty((n,K))\n",
    "    for k in range(K):\n",
    "        D = X - mus[k]\n",
    "        S_inv = Sigmas_inv[k]\n",
    "        quad = np.array([d_i @ S_inv @ d_i for d_i in D])   # bucle por fila\n",
    "        out[:,k] = -0.5*(quad + logdets[k]) + log_priors[k]\n",
    "    return out\n",
    "\n",
    "def qda_scores_vector_diag(X, mus, Sigmas_inv, logdets, log_priors):\n",
    "    n, d = X.shape\n",
    "    K = len(mus)\n",
    "    out = np.empty((n,K))\n",
    "    for k in range(K):\n",
    "        D = X - mus[k]\n",
    "        S_inv = Sigmas_inv[k]\n",
    "        M = D @ S_inv @ D.T                 # (n,n) ¡costoso!\n",
    "        quad = np.diag(M)\n",
    "        out[:,k] = -0.5*(quad + logdets[k]) + log_priors[k]\n",
    "    return out\n",
    "\n",
    "# FasterQDA\n",
    "def qda_scores_faster(X, mus, Sigmas_inv, logdets, log_priors):\n",
    "    return qda_scores_efficient(X, mus, Sigmas_inv, logdets, log_priors)\n",
    "\n",
    "# EfficientQDA: ya definida arriba (qda_scores_efficient)\n",
    "\n",
    "# Benchmark\n",
    "X, mus, Sigmas, Sigmas_inv, logdets, log_priors = make_synth()\n",
    "\n",
    "def bench(fn, name):\n",
    "    t0 = perf_counter()\n",
    "    S = fn(X, mus, Sigmas_inv, logdets, log_priors)\n",
    "    dt = perf_counter() - t0\n",
    "    y = np.argmax(S, axis=1)\n",
    "    return name, dt, y, S\n",
    "\n",
    "res = []\n",
    "for fn, name in [\n",
    "    (qda_scores_naive_loop, \"Naive loop\"),\n",
    "    (qda_scores_vector_diag, \"Vector + diag(nxn)\"),\n",
    "    (qda_scores_faster,     \"FasterQDA\"),\n",
    "    (qda_scores_efficient,  \"EfficientQDA\"),\n",
    "]:\n",
    "    name, dt, y, S = bench(fn, name)\n",
    "    res.append((name, dt, y, S))\n",
    "    print(f\"{name:20s} {dt:8.3f} s\")\n",
    "\n",
    "# Comprobaciones de igualdad numérica\n",
    "base_y = res[0][2]\n",
    "for name, _, y, S in res[1:]:\n",
    "    assert np.all(y == base_y), f\"Etiquetas distintas en {name}\"\n",
    "    # tolerancia por FP:\n",
    "    assert np.allclose(S, res[0][3], rtol=1e-6, atol=1e-8), f\"Puntajes difieren en {name}\"\n",
    "print(\"✓ Todas las variantes dan el mismo resultado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enunciado N°8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expresar A⁻¹ en términos de L\n",
    "\n",
    "**Demostración**\n",
    "\n",
    "Si una matriz $A$ tiene factorización de Cholesky $A = LL^T$, entonces:\n",
    "$$A^{-1} = (LL^T)^{-1} = (L^T)^{-1}L^{-1} = (L^{-1})^T L^{-1}$$\n",
    "\n",
    "Justificación:\n",
    "\n",
    "- Por propiedad de inversas: $(AB)^{-1} = B^{-1}A^{-1}$\n",
    "- Aplicando a $A = LL^T$: $A^{-1} = (LL^T)^{-1} = (L^T)^{-1}L^{-1}$\n",
    "- Como $L^{-1}$ es triangular inferior, $(L^{-1})^T$ es triangular superior\n",
    "- Por lo tanto: $A^{-1} = (L^{-1})^T L^{-1}$\n",
    "\n",
    "**Utilidad en QDA**\n",
    "\n",
    "Para la forma cuadrática:\n",
    "\n",
    "En QDA tenemos:\n",
    "\n",
    "$$(x-\\mu_j)^T \\Sigma_j^{-1} (x-\\mu_j)$$\n",
    "Si $\\Sigma_j = LL^T$, entonces $\\Sigma_j^{-1} = (L^{-1})^T L^{-1}$, y podemos escribir:\n",
    "$$(x-\\mu_j)^T (L^{-1})^T L^{-1} (x-\\mu_j) = \\|L^{-1}(x-\\mu_j)\\|^2$$\n",
    "Definiendo $y = L^{-1}(x-\\mu_j)$, la forma cuadrática se reduce a:\n",
    "$$\\|y\\|^2 = y^Ty = \\sum_{i=1}^p y_i^2$$\n",
    "que es mucho más simple y eficiente de calcular.\n",
    "\n",
    "Para el determinante:\n",
    "\n",
    "$$|\\Sigma_j^{-1}| = |(L^{-1})^T L^{-1}| = |L^{-1}|^2 = \\left(\\prod_{i=1}^p (L^{-1})_{ii}\\right)^2$$\n",
    "Por lo tanto:\n",
    "$$\\log|\\Sigma_j^{-1}| = 2\\log\\left(\\prod_{i=1}^p (L^{-1}){ii}\\right) = 2\\sum{i=1}^p \\log(L^{-1})_{ii}$$\n",
    "O equivalentemente, si trabajamos con $L$ en lugar de $L^{-1}$:\n",
    "$$\\log|\\Sigma_j^{-1}| = -2\\sum_{i=1}^p \\log L_{ii}$$\n",
    "\n",
    "**Ventajas computacionales:**\n",
    "1. Evita calcular explícitamente $\\Sigma_j^{-1}$ (matriz completa $p \\times p$)\n",
    "2. Reduce la forma cuadrática a un producto escalar simple \n",
    "3. El determinante se calcula solo con los elementos de la diagonal\n",
    "4. Aprovecha la estructura triangular de $L$ para operaciones más eficientes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enunciado N°9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diferencias entre `QDA` y `QDA_Chol1`\n",
    "\n",
    "##### Diferencias principales\n",
    "\n",
    "**`QDA` (implementación directa):**\n",
    "- Calcula e invierte directamente la matriz de covarianza: `Σ_j^{-1} = LA.inv(np.cov(...))`\n",
    "- Almacena `inv_covs` (las matrices inversas completas `Σ_j^{-1}`)\n",
    "- Calcula el determinante de `Σ_j^{-1}` en cada predicción: `LA.det(inv_cov)`\n",
    "- Forma cuadrática: `unbiased_x.T @ inv_cov @ unbiased_x`\n",
    "\n",
    "**`QDA_Chol1` (usando descomposición de Cholesky):**\n",
    "- Factoriza la matriz de covarianza usando Cholesky: `L_j = cholesky(Σ_j, lower=True)` donde `Σ_j = L_j @ L_j^T`\n",
    "- Invierte la matriz triangular de Cholesky: `L_inv_j = LA.inv(L_j)`\n",
    "- Almacena `L_invs` (las inversas de las factorizaciones de Cholesky `L_j^{-1}`)\n",
    "- Calcula el determinante usando la diagonal: `prod(diag(L_inv_j))`\n",
    "- Forma cuadrática: `||L_inv @ unbiased_x||^2`\n",
    "\n",
    "##### Paso a paso: Cómo `QDA_Chol1` llega a las predicciones\n",
    "\n",
    "**Fase 1: Entrenamiento (`_fit_params`)**\n",
    "\n",
    "Para cada clase `idx = 0, 1, ..., k-1`:\n",
    "\n",
    "1. **Filtrar datos de la clase**: `X[:,y.flatten()==idx]` (columnas de X correspondientes a clase idx)\n",
    "\n",
    "2. **Calcular matriz de covarianza**: `Σ_idx = np.cov(X[:,y.flatten()==idx], bias=True)` (matriz p×p)\n",
    "\n",
    "3. **Descomposición de Cholesky**: `L_idx = cholesky(Σ_idx, lower=True)` donde `Σ_idx = L_idx @ L_idx^T` y `L_idx` es triangular inferior\n",
    "\n",
    "4. **Invertir la matriz triangular de Cholesky**: `L_inv_idx = LA.inv(L_idx)` (más eficiente que invertir `Σ_idx` directamente)\n",
    "\n",
    "5. **Calcular media de la clase**: `μ_idx = X[:,y.flatten()==idx].mean(axis=1, keepdims=True)` (vector p×1)\n",
    "\n",
    "6. **Almacenar en listas**: `self.L_invs = [L_inv_0, L_inv_1, ..., L_inv_{k-1}]` y `self.means = [μ_0, μ_1, ..., μ_{k-1}]`\n",
    "\n",
    "**Fase 2: Predicción (`_predict_log_conditional`)**\n",
    "\n",
    "Para una observación `x` y clase candidata `class_idx`:\n",
    "\n",
    "1. **Recuperar parámetros de la clase**: `L_inv = self.L_invs[class_idx]` (matriz p×p triangular inferior invertida) y `μ = self.means[class_idx]` (vector p×1)\n",
    "\n",
    "2. **Centrar la observación**: `unbiased_x = x - μ` (vector p×1: `(x - μ_j)`)\n",
    "\n",
    "3. **Transformar usando `L^{-1}`**: `y = L_inv @ unbiased_x` (vector p×1)\n",
    "\n",
    "   **Relación matemática clave:** En `QDA` calculábamos la forma cuadrática: `(x-μ_j)^T Σ_j^{-1} (x-μ_j)`.\n",
    "   \n",
    "    Como `Σ_j = L_j @ L_j^T`, entonces `Σ_j^{-1} = (L_j^T)^{-1} @ L_j^{-1} = (L_j^{-1})^T @ L_j^{-1}`.\n",
    "    \n",
    "    Por lo tanto: `(x-μ_j)^T Σ_j^{-1} (x-μ_j) = (x-μ_j)^T (L_inv^T @ L_inv) (x-μ_j) = (L_inv @ (x-μ_j))^T @ (L_inv @ (x-μ_j)) = ||y||^2`\n",
    "\n",
    "4. **Calcular log-determinante**: `log_det_term = np.log(L_inv.diagonal().prod())`\n",
    "\n",
    "   **Justificación:** Para matrices triangulares: `det(L_inv) = prod(diag(L_inv))`. Como `det(Σ_j^{-1}) = det((L_j^{-1})^T @ L_j^{-1}) = det(L_j^{-1})^2`, entonces: `log(|Σ_j^{-1}|^{1/2}) = log(det(L_j^{-1})) = log(prod(diag(L_inv)))`\n",
    "\n",
    "5. **Calcular distancia de Mahalanobis al cuadrado**: `mahalanobis_sq = (y**2).sum()` que equivale a `y^T @ y = ||y||^2`\n",
    "\n",
    "6. **Retornar log-densidad condicional**: `return np.log(L_inv.diagonal().prod()) - 0.5 * (y**2).sum()` que corresponde a: `log f_j(x) = (1/2)log|Σ_j^{-1}| - (1/2)(x-μ_j)^T Σ_j^{-1} (x-μ_j) + C`\n",
    "\n",
    "**Fase 3: Clasificación (heredada de `BaseBayesianClassifier`)**\n",
    "\n",
    "Para cada observación, el modelo: (1) Calcula `log f_j(x) + log π_j` para todas las clases `j` (en `_predict_one`), (2) Retorna `argmax_j { log f_j(x) + log π_j }`\n",
    "\n",
    "##### Ventajas de `QDA_Chol1` sobre `QDA`\n",
    "\n",
    "1. **Estabilidad numérica**: La descomposición de Cholesky es más estable que invertir directamente `Σ_j`\n",
    "2. **Eficiencia en inversión**: Invertir una matriz triangular (`L_j`) es más rápido que invertir una matriz densa (`Σ_j`)\n",
    "3. **Determinante eficiente**: El determinante se obtiene como producto de la diagonal (operación O(p) vs O(p³))\n",
    "4. **Forma cuadrática optimizada**: Calcular `||L_inv @ v||^2` requiere una sola multiplicación matriz-vector seguida de un producto punto, más eficiente que `v^T @ Σ_inv @ v`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enunciado N°10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diferencias entre `QDA_Chol1`, `QDA_Chol2` y `QDA_Chol3`\n",
    "\n",
    "Las tres implementaciones usan la descomposición de Cholesky pero de formas distintas:\n",
    "\n",
    "##### `QDA_Chol1` - Invierte L con NumPy\n",
    "\n",
    "**Entrenamiento:**\n",
    "```python\n",
    "L = cholesky(Σ_j)           # Factoriza: Σ = L·L^T\n",
    "L_inv = LA.inv(L)           # Invierte L usando NumPy\n",
    "```\n",
    "**Predicción:**\n",
    "```python\n",
    "y = L_inv @ (x - μ_j)       # Multiplica directamente\n",
    "```\n",
    "**Ventaja:** simple, usa solo NumPy.\n",
    "\n",
    "**Desventaja:** LA.inv*() no aprovecha que L es triangular.\n",
    "\n",
    "##### `QDA_Chol2` - NO invierte, resuelve sistemas\n",
    "\n",
    "**Entrenamiento:**\n",
    "```python\n",
    "L = cholesky(Σ_j)           # Solo factoriza, NO invierte\n",
    "```\n",
    "**Predicción:**\n",
    "```python\n",
    "y = solve_triangular(L, x - μ_j)  # Resuelve L·y = (x-μ) sin invertir\n",
    "```\n",
    "**Ventaja:** Entrenamiento más rápido (no invierte)  \n",
    "\n",
    "**Desventaja:** Predicción más lenta (resuelve sistema cada vez)\n",
    "\n",
    "##### `QDA_Chol3` - Invierte L con LAPACK optimizado\n",
    "\n",
    "**Entrenamiento:**\n",
    "```python\n",
    "L = cholesky(Σ_j)           # Factoriza\n",
    "L_inv = dtrtri(L)           # Invierte usando LAPACK (optimizado para triangulares)\\\n",
    "```\n",
    "**Predicción:**\n",
    "```python\n",
    "y = L_inv @ (x - μ_j)       # Multiplica directamente\n",
    "```\n",
    "**Ventaja:** Inversión más eficiente que NumPy, predicción rápida.\n",
    "\n",
    "**Desventaja:** Entrenamiento más lento que Chol2.\n",
    "\n",
    "##### En resumen\n",
    "\n",
    "- **Chol1:** Invierte con NumPy (opción intermedia)\n",
    "- **Chol2:** No invierte, resuelve cada vez (entrena rápido, predice lento)\n",
    "- **Chol3:** Invierte con LAPACK (entrena lento, predice rápido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enunciado N°11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benching params:\n",
      "Total runs: 600\n",
      "Warmup runs: 50\n",
      "Peak Memory usage runs: 50\n",
      "Running time runs: 500\n",
      "Train size rows (approx): 125\n",
      "Test size rows (approx): 53\n",
      "Test size fraction: 0.3\n",
      "Benchmarking 7 variantes de QDA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QDA (MEM): 100%|██████████| 50/50 [00:00<00:00, 184.59it/s]\n",
      "QDA (TIME): 100%|██████████| 500/500 [00:00<00:00, 568.59it/s]\n",
      "TensorizedQDA (MEM): 100%|██████████| 50/50 [00:00<00:00, 395.36it/s]\n",
      "TensorizedQDA (TIME): 100%|██████████| 500/500 [00:00<00:00, 1009.23it/s]\n",
      "FasterQDA (MEM): 100%|██████████| 50/50 [00:00<00:00, 1286.19it/s]\n",
      "FasterQDA (TIME): 100%|██████████| 500/500 [00:00<00:00, 2761.97it/s]\n",
      "EfficientQDA (MEM): 100%|██████████| 50/50 [00:00<00:00, 1187.88it/s]\n",
      "EfficientQDA (TIME): 100%|██████████| 500/500 [00:00<00:00, 2833.55it/s]\n",
      "QDA_Chol1 (MEM): 100%|██████████| 50/50 [00:00<00:00, 277.86it/s]\n",
      "QDA_Chol1 (TIME): 100%|██████████| 500/500 [00:00<00:00, 836.27it/s]\n",
      "QDA_Chol2 (MEM): 100%|██████████| 50/50 [00:00<00:00, 121.37it/s]\n",
      "QDA_Chol2 (TIME): 100%|██████████| 500/500 [00:01<00:00, 441.91it/s]\n",
      "QDA_Chol3 (MEM): 100%|██████████| 50/50 [00:00<00:00, 291.90it/s]\n",
      "QDA_Chol3 (TIME): 100%|██████████| 500/500 [00:00<00:00, 894.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RESUMEN COMPLETO DE PERFORMANCE\n",
      "================================================================================\n",
      "               train_median_ms  train_std_ms  test_median_ms  test_std_ms  \\\n",
      "model                                                                       \n",
      "QDA                   0.165216      0.119483        1.212323     0.463007   \n",
      "TensorizedQDA         0.166197      0.155533        0.550152     0.205292   \n",
      "FasterQDA             0.153904      0.104913        0.037887     0.024708   \n",
      "EfficientQDA          0.145733      0.102384        0.037342     0.036409   \n",
      "QDA_Chol1             0.198568      0.110739        0.693402     0.357725   \n",
      "QDA_Chol2             0.168980      0.133044        1.662823     0.487540   \n",
      "QDA_Chol3             0.156212      0.107339        0.685636     0.253093   \n",
      "\n",
      "               mean_accuracy  train_mem_median_mb  train_mem_std_mb  \\\n",
      "model                                                                 \n",
      "QDA                 0.984148             0.017933          0.000989   \n",
      "TensorizedQDA       0.984148             0.017933          0.000982   \n",
      "FasterQDA           0.984148             0.017933          0.000958   \n",
      "EfficientQDA        0.984148             0.017933          0.000985   \n",
      "QDA_Chol1           0.984148             0.018101          0.000978   \n",
      "QDA_Chol2           0.984148             0.017956          0.000982   \n",
      "QDA_Chol3           0.984148             0.017956          0.001004   \n",
      "\n",
      "               test_mem_median_mb  test_mem_std_mb  train_speedup  \\\n",
      "model                                                               \n",
      "QDA                      0.007560         0.000072       1.000000   \n",
      "TensorizedQDA            0.011986         0.000017       0.994097   \n",
      "FasterQDA                0.075035         0.000035       1.073504   \n",
      "EfficientQDA             0.075035         0.000036       1.133686   \n",
      "QDA_Chol1                0.007767         0.000049       0.832037   \n",
      "QDA_Chol2                0.007968         0.000086       0.977728   \n",
      "QDA_Chol3                0.007622         0.000059       1.057640   \n",
      "\n",
      "               test_speedup  train_mem_reduction  test_mem_reduction  \n",
      "model                                                                 \n",
      "QDA                1.000000             1.000000            1.000000  \n",
      "TensorizedQDA      2.203613             1.000000            0.630729  \n",
      "FasterQDA         31.998389             1.000000            0.100750  \n",
      "EfficientQDA      32.464967             1.000000            0.100750  \n",
      "QDA_Chol1          1.748371             0.990727            0.973355  \n",
      "QDA_Chol2          0.729075             0.998725            0.948773  \n",
      "QDA_Chol3          1.768173             0.998725            0.991867  \n",
      "\n",
      "================================================================================\n",
      "ANÁLISIS DE TIEMPOS\n",
      "================================================================================\n",
      "               train_median_ms  train_std_ms  test_median_ms  test_std_ms  \\\n",
      "model                                                                       \n",
      "QDA                     0.1652        0.1195          1.2123       0.4630   \n",
      "TensorizedQDA           0.1662        0.1555          0.5502       0.2053   \n",
      "FasterQDA               0.1539        0.1049          0.0379       0.0247   \n",
      "EfficientQDA            0.1457        0.1024          0.0373       0.0364   \n",
      "QDA_Chol1               0.1986        0.1107          0.6934       0.3577   \n",
      "QDA_Chol2               0.1690        0.1330          1.6628       0.4875   \n",
      "QDA_Chol3               0.1562        0.1073          0.6856       0.2531   \n",
      "\n",
      "               train_speedup  test_speedup  \n",
      "model                                       \n",
      "QDA                   1.0000        1.0000  \n",
      "TensorizedQDA         0.9941        2.2036  \n",
      "FasterQDA             1.0735       31.9984  \n",
      "EfficientQDA          1.1337       32.4650  \n",
      "QDA_Chol1             0.8320        1.7484  \n",
      "QDA_Chol2             0.9777        0.7291  \n",
      "QDA_Chol3             1.0576        1.7682  \n",
      "\n",
      "================================================================================\n",
      "ANÁLISIS DE MEMORIA\n",
      "================================================================================\n",
      "               train_mem_median_mb  train_mem_std_mb  test_mem_median_mb  \\\n",
      "model                                                                      \n",
      "QDA                         0.0179             0.001              0.0076   \n",
      "TensorizedQDA               0.0179             0.001              0.0120   \n",
      "FasterQDA                   0.0179             0.001              0.0750   \n",
      "EfficientQDA                0.0179             0.001              0.0750   \n",
      "QDA_Chol1                   0.0181             0.001              0.0078   \n",
      "QDA_Chol2                   0.0180             0.001              0.0080   \n",
      "QDA_Chol3                   0.0180             0.001              0.0076   \n",
      "\n",
      "               test_mem_std_mb  train_mem_reduction  test_mem_reduction  \n",
      "model                                                                    \n",
      "QDA                     0.0001               1.0000              1.0000  \n",
      "TensorizedQDA           0.0000               1.0000              0.6307  \n",
      "FasterQDA               0.0000               1.0000              0.1007  \n",
      "EfficientQDA            0.0000               1.0000              0.1007  \n",
      "QDA_Chol1               0.0000               0.9907              0.9734  \n",
      "QDA_Chol2               0.0001               0.9987              0.9488  \n",
      "QDA_Chol3               0.0001               0.9987              0.9919  \n",
      "\n",
      "================================================================================\n",
      "ANÁLISIS DE ACCURACY\n",
      "================================================================================\n",
      "               mean_accuracy\n",
      "model                       \n",
      "QDA                 0.984148\n",
      "TensorizedQDA       0.984148\n",
      "FasterQDA           0.984148\n",
      "EfficientQDA        0.984148\n",
      "QDA_Chol1           0.984148\n",
      "QDA_Chol2           0.984148\n",
      "QDA_Chol3           0.984148\n",
      "\n",
      "================================================================================\n",
      "RANKING DE MODELOS\n",
      "================================================================================\n",
      "✓ Más rápido en ENTRENAMIENTO: EfficientQDA (0.1457 ms)\n",
      "✓ Más rápido en PREDICCIÓN: EfficientQDA (0.0373 ms)\n",
      "✓ Menos memoria en ENTRENAMIENTO: QDA (0.0179 MB)\n",
      "✓ Menos memoria en PREDICCIÓN: QDA (0.0076 MB)\n",
      "✗ Más lento en ENTRENAMIENTO: QDA_Chol1 (0.1986 ms)\n",
      "✗ Más lento en PREDICCIÓN: QDA_Chol2 (1.6628 ms)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Comparación de las 7 variantes de QDA\n",
    "\n",
    "# Primero, aseguramos tener el dataset cargado\n",
    "X_full, y_full = get_wine_dataset()\n",
    "y_full_encoded = label_encode(y_full)\n",
    "\n",
    "# Creamos un benchmark con más runs para tener resultados más confiables\n",
    "b_chol = Benchmark(\n",
    "    X_full, y_full_encoded,\n",
    "    n_runs = 500,      # Más runs para mejor precisión\n",
    "    warmup = 50,\n",
    "    mem_runs = 50,\n",
    "    test_sz = 0.3,\n",
    "    same_splits = True  # Mismo split para comparación justa\n",
    ")\n",
    "\n",
    "# Bencheamos las 7 variantes\n",
    "modelos_a_comparar = [\n",
    "    QDA,              # 1. Original\n",
    "    TensorizedQDA,    # 2. Tensorizado sobre clases\n",
    "    FasterQDA,        # 3. Elimina for en predict\n",
    "    EfficientQDA,     # 4. Eficiente sin matriz n×n\n",
    "    QDA_Chol1,        # 5. Cholesky con LA.inv\n",
    "    QDA_Chol2,        # 6. Cholesky con solve_triangular\n",
    "    QDA_Chol3         # 7. Cholesky con dtrtri\n",
    "]\n",
    "\n",
    "print(\"Benchmarking 7 variantes de QDA...\")\n",
    "for modelo in modelos_a_comparar:\n",
    "    b_chol.bench(modelo)\n",
    "\n",
    "# Generamos el resumen completo\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESUMEN COMPLETO DE PERFORMANCE\")\n",
    "print(\"=\"*80)\n",
    "summ_completo = b_chol.summary(baseline='QDA')\n",
    "print(summ_completo)\n",
    "\n",
    "# Análisis específico: tiempos de entrenamiento y predicción\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANÁLISIS DE TIEMPOS\")\n",
    "print(\"=\"*80)\n",
    "tiempos = summ_completo[[\n",
    "    'train_median_ms', 'train_std_ms', \n",
    "    'test_median_ms', 'test_std_ms',\n",
    "    'train_speedup', 'test_speedup'\n",
    "]].round(4)\n",
    "print(tiempos)\n",
    "\n",
    "# Análisis específico: uso de memoria\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANÁLISIS DE MEMORIA\")\n",
    "print(\"=\"*80)\n",
    "memoria = summ_completo[[\n",
    "    'train_mem_median_mb', 'train_mem_std_mb',\n",
    "    'test_mem_median_mb', 'test_mem_std_mb',\n",
    "    'train_mem_reduction', 'test_mem_reduction'\n",
    "]].round(4)\n",
    "print(memoria)\n",
    "\n",
    "# Análisis específico: accuracy\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANÁLISIS DE ACCURACY\")\n",
    "print(\"=\"*80)\n",
    "accuracy = summ_completo[['mean_accuracy']].round(6)\n",
    "print(accuracy)\n",
    "\n",
    "# Identificar mejor y peor modelo\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RANKING DE MODELOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Mejor en entrenamiento\n",
    "mejor_train = summ_completo['train_median_ms'].idxmin()\n",
    "print(f\"✓ Más rápido en ENTRENAMIENTO: {mejor_train} ({summ_completo.loc[mejor_train, 'train_median_ms']:.4f} ms)\")\n",
    "\n",
    "# Mejor en predicción\n",
    "mejor_test = summ_completo['test_median_ms'].idxmin()\n",
    "print(f\"✓ Más rápido en PREDICCIÓN: {mejor_test} ({summ_completo.loc[mejor_test, 'test_median_ms']:.4f} ms)\")\n",
    "\n",
    "# Mejor en memoria de entrenamiento\n",
    "mejor_mem_train = summ_completo['train_mem_median_mb'].idxmin()\n",
    "print(f\"✓ Menos memoria en ENTRENAMIENTO: {mejor_mem_train} ({summ_completo.loc[mejor_mem_train, 'train_mem_median_mb']:.4f} MB)\")\n",
    "\n",
    "# Mejor en memoria de predicción\n",
    "mejor_mem_test = summ_completo['test_mem_median_mb'].idxmin()\n",
    "print(f\"✓ Menos memoria en PREDICCIÓN: {mejor_mem_test} ({summ_completo.loc[mejor_mem_test, 'test_mem_median_mb']:.4f} MB)\")\n",
    "\n",
    "# Peor en entrenamiento\n",
    "peor_train = summ_completo['train_median_ms'].idxmax()\n",
    "print(f\"✗ Más lento en ENTRENAMIENTO: {peor_train} ({summ_completo.loc[peor_train, 'train_median_ms']:.4f} ms)\")\n",
    "\n",
    "# Peor en predicción\n",
    "peor_test = summ_completo['test_median_ms'].idxmax()\n",
    "print(f\"✗ Más lento en PREDICCIÓN: {peor_test} ({summ_completo.loc[peor_test, 'test_median_ms']:.4f} ms)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análisis de Performance de las 7 Variantes de QDA\n",
    "\n",
    "##### Observaciones Generales\n",
    "\n",
    "**Accuracy:**\n",
    "- Todas las variantes mantienen el mismo accuracy (≈98.4%), lo cual es esperado ya que implementan el mismo algoritmo matemático, solo difieren en la implementación computacional.\n",
    "\n",
    "**Tiempos de Entrenamiento:**\n",
    "- Las diferencias son pequeñas (~0.04 ms entre la más rápida y más lenta)\n",
    "- `EfficientQDA` es marginalmente la más rápida (0.1457 ms)\n",
    "- `QDA_Chol1` es la más lenta (0.1986 ms) porque invierte L con `LA.inv` sin aprovechar que es triangular\n",
    "\n",
    "**Tiempos de Predicción:**\n",
    "- Aquí sí hay diferencias **dramáticas** (factor 32x entre extremos)\n",
    "- Las variantes vectorizadas (`FasterQDA`, `EfficientQDA`) son **~32x más rápidas** que QDA base\n",
    "- `QDA_Chol2` es la **más lenta** (1.66 ms) porque resuelve un sistema triangular por cada observación\n",
    "\n",
    "**Uso de Memoria:**\n",
    "- Entrenamiento: Todas usan memoria similar (~0.018 MB)\n",
    "- Predicción: Las variantes vectorizadas usan **~10x más memoria** (0.075 MB vs 0.0076 MB) porque procesan todas las observaciones simultáneamente\n",
    "\n",
    "##### Comparación entre implementaciones de Cholesky\n",
    "\n",
    "**`QDA_Chol1` (LA.inv sobre L):**\n",
    "- ✗ Entrenamiento más lento (0.1986 ms)\n",
    "- ✓ Predicción rápida (0.6934 ms)\n",
    "- ✗ No aprovecha que L es triangular\n",
    "- **Conclusión:** No tiene ventajas sobre Chol3\n",
    "\n",
    "**`QDA_Chol2` (solve_triangular):**\n",
    "- ✓ Entrenamiento intermedio (0.1690 ms)\n",
    "- ✗✗ Predicción **MÁS LENTA de todas** (1.6628 ms)\n",
    "- ✗ Resuelve sistema triangular en cada predicción\n",
    "- **Conclusión:** Claramente la peor opción\n",
    "\n",
    "**`QDA_Chol3` (dtrtri):**\n",
    "- ✓ Entrenamiento rápido (0.1562 ms)\n",
    "- ✓ Predicción rápida (0.6856 ms)\n",
    "- ✓ Inversión optimizada para matrices triangulares\n",
    "- ✓ Pre-calcula la inversa una sola vez\n",
    "- **Conclusión:** Mejor implementación de Cholesky\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enunciado N°12 y N°13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " DATOS\n",
      "  - Muestras: 5000\n",
      "  - Features: 50\n",
      "  - Clases: 3\n",
      "==================================================\n",
      "BENCHMARK: Implementación QDA \n",
      "==================================================\n",
      "\n",
      "1. ENTRENAMIENTO\n",
      "--------------------------------------------------\n",
      "QDA_Efficient       : 3.7071 ms\n",
      "QDA_Chol            : 2.0015 ms\n",
      "TensorizedChol      : 1.4094 ms\n",
      "\n",
      "2. PREDICCION\n",
      "--------------------------------------------------\n",
      "QDA_Efficient       : 5.4545 ms\n",
      "QDA_Chol            : 243.9210 ms\n",
      "TensorizedChol      : 6.9141 ms\n",
      "\n",
      "3. VERIFICACION: Dan el mismo resultado?\n",
      "--------------------------------------------------\n",
      "✓ QDA_Chol             ≡ QDA_Efficient: Pred=True, Scores=True\n",
      "✓ TensorizedChol       ≡ QDA_Efficient: Pred=True, Scores=True\n",
      "\n",
      "4. SPEEDUP\n",
      "--------------------------------------------------\n",
      "QDA_Efficient       : 1.00x\n",
      "QDA_Chol            : 0.02x\n",
      "TensorizedChol      : 0.79x\n",
      "\n",
      "==================================================\n",
      "RESULTADO\n",
      "==================================================\n",
      "✓ TensorizedChol paraleliza correctamente\n",
      "✓ Speedup logrado: 0.79x\n",
      "✓ Tiempo de predicción: 6.9141 ms\n",
      "✓ Precisión: Idéntica a baseline\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import solve_triangular, cholesky\n",
    "from time import perf_counter\n",
    "\n",
    "\n",
    "class QDA_Efficient: # clase base (hereda la logica de qda_scores_efficient)\n",
    "#implementacion QDA usando inversion directa, base para comparacion\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.mus = None\n",
    "        self.Sigmas_inv = None\n",
    "        self.logdets = None\n",
    "        self.log_priors = None\n",
    "        self.K = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "    # X: (n, d)\n",
    "    # y: (n,) etiquetas de clase 0 a K-1\n",
    "        self.K = len(np.unique(y))\n",
    "        self.mus = []\n",
    "        self.Sigmas_inv = []\n",
    "        self.logdets = np.zeros(self.K)\n",
    "        \n",
    "        for k in range(self.K):\n",
    "            X_k = X[y == k]\n",
    "            \n",
    "            # media\n",
    "            mu_k = np.mean(X_k, axis=0)\n",
    "            self.mus.append(mu_k)\n",
    "            \n",
    "            # covarianza\n",
    "            Sigma_k = np.cov(X_k.T)\n",
    "            if Sigma_k.ndim == 0:  # si es escalar (una sola feature)\n",
    "                Sigma_k = np.array([[Sigma_k]])\n",
    "            \n",
    "            # inversa\n",
    "            self.Sigmas_inv.append(np.linalg.inv(Sigma_k))\n",
    "            \n",
    "            # log determinante\n",
    "            self.logdets[k] = np.log(np.linalg.det(Sigma_k))\n",
    "        \n",
    "        # Priors uniformes\n",
    "        self.log_priors = np.log(np.ones(self.K) / self.K)\n",
    "    \n",
    "    def qda_scores_efficient(self, X):\n",
    "        \n",
    "        #implementacion original eficiente con inversion\n",
    "        n, d = X.shape\n",
    "        scores = np.empty((n, self.K), dtype=float)\n",
    "        \n",
    "        for k in range(self.K):\n",
    "            mu = self.mus[k]\n",
    "            S_inv = self.Sigmas_inv[k]\n",
    "            D = X - mu\n",
    "            quad = np.sum((D @ S_inv) * D, axis=1)\n",
    "            scores[:, k] = -0.5 * (quad + self.logdets[k]) + self.log_priors[k]\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def predict(self, X):\n",
    "        scores = self.qda_scores_efficient(X)\n",
    "        y_hat = np.argmax(scores, axis=1)\n",
    "        return y_hat, scores\n",
    "\n",
    "# version CHOLESKY base: QDA_Chol\n",
    "class QDA_Chol(QDA_Efficient):\n",
    "#QDA usando descomposicion Cholesky, mas estable numericamente que inversion directa \n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Ls = None  # Factores Cholesky\n",
    "    \n",
    "    def fit(self, X, y): #entrenamiento: calcular L_k para cada clase\n",
    "        self.K = len(np.unique(y))\n",
    "        self.mus = []\n",
    "        self.Ls = []\n",
    "        self.logdets = np.zeros(self.K)\n",
    "        \n",
    "        for k in range(self.K):\n",
    "            X_k = X[y == k]\n",
    "            \n",
    "            # media\n",
    "            mu_k = np.mean(X_k, axis=0)\n",
    "            self.mus.append(mu_k)\n",
    "            \n",
    "            # covarianza\n",
    "            Sigma_k = np.cov(X_k.T)\n",
    "            if Sigma_k.ndim == 0:\n",
    "                Sigma_k = np.array([[Sigma_k]])\n",
    "            \n",
    "            # descomposicion Cholesky\n",
    "            L_k = cholesky(Sigma_k, lower=True)\n",
    "            self.Ls.append(L_k)\n",
    "            self.logdets[k] = 2 * np.sum(np.log(np.diag(L_k)))\n",
    "                    \n",
    "        # priors uniformes\n",
    "        self.log_priors = np.log(np.ones(self.K) / self.K)\n",
    "    \n",
    "    def qda_scores_chol(self, X):\n",
    "        #scores usando Cholesky sin paralelizar\n",
    "        n, d = X.shape\n",
    "        scores = np.empty((n, self.K), dtype=float)\n",
    "        \n",
    "        for k in range(self.K):\n",
    "            D = X - self.mus[k]  # (n, d)\n",
    "            L_k = self.Ls[k]     # (d, d)\n",
    "            \n",
    "            # resolver para cada observacion\n",
    "            quad = np.zeros(n)\n",
    "            for i in range(n):\n",
    "                z_i = solve_triangular(L_k, D[i], lower=True)\n",
    "                quad[i] = np.sum(z_i**2)\n",
    "            \n",
    "            scores[:, k] = -0.5 * (quad + self.logdets[k]) + self.log_priors[k]\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def predict(self, X):\n",
    "        scores = self.qda_scores_chol(X)\n",
    "        y_hat = np.argmax(scores, axis=1)\n",
    "        return y_hat, scores\n",
    "    \n",
    "    \n",
    "\n",
    "# version paralelizada: TensorizedChol\n",
    "\n",
    "class TensorizedChol(QDA_Chol): #QDA con Cholesky paralelizado sobre observaciones\n",
    "    def qda_scores_tensorized(self, X):\n",
    "        n, d = X.shape\n",
    "        scores = np.empty((n, self.K), dtype=float)\n",
    "        \n",
    "        for k in range(self.K):\n",
    "            D = X - self.mus[k]  # (n, d)\n",
    "            L_k = self.Ls[k]     # (d, d) triangular inferior\n",
    "            Z = solve_triangular(L_k, D.T, lower=True)  # (d, n)\n",
    "            quad = np.sum(Z**2, axis=0)  # (n,)\n",
    "            scores[:, k] = -0.5 * (quad + self.logdets[k]) + self.log_priors[k]\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def predict(self, X):\n",
    "        #Prediccion usando version paralelizada\n",
    "        scores = self.qda_scores_tensorized(X)\n",
    "        y_hat = np.argmax(scores, axis=1)\n",
    "        return y_hat, scores\n",
    "    \n",
    "    \n",
    "# funcion de BENCHMARK\n",
    "    #Comparar velocidades de:\n",
    "    #1) QDA_Efficient (inversion directa)\n",
    "    #2) QDA_Chol (Cholesky sin paralelizar)\n",
    "    #3) TensorizedChol (Cholesky paralelizado)\n",
    "    \n",
    "def benchmark_qda_implementations(X, y, n_runs=10):\n",
    "    \n",
    "    models = {\n",
    "        'QDA_Efficient': QDA_Efficient(),\n",
    "        'QDA_Chol': QDA_Chol(),\n",
    "        'TensorizedChol': TensorizedChol()\n",
    "    }\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    print(\"BENCHMARK: Implementación QDA \")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # entrenamiento\n",
    "    print(\"\\n1. ENTRENAMIENTO\")\n",
    "    print(\"-\"*50)\n",
    "    train_times = {}\n",
    "    for name, model in models.items():\n",
    "        times = []\n",
    "        for _ in range(n_runs):\n",
    "            t0 = perf_counter()\n",
    "            model.fit(X, y)\n",
    "            times.append(perf_counter() - t0)\n",
    "        avg_time = np.mean(times)\n",
    "        train_times[name] = avg_time\n",
    "        print(f\"{name:20s}: {avg_time*1000:.4f} ms\")\n",
    "    \n",
    "    # prediccion\n",
    "    print(\"\\n2. PREDICCION\")\n",
    "    print(\"-\"*50)\n",
    "    results = {}\n",
    "    predict_times = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        times = []\n",
    "        for _ in range(n_runs):\n",
    "            t0 = perf_counter()\n",
    "            y_pred, scores = model.predict(X)\n",
    "            times.append(perf_counter() - t0)\n",
    "        \n",
    "        avg_time = np.mean(times)\n",
    "        predict_times[name] = avg_time\n",
    "        results[name] = (y_pred, scores)\n",
    "        print(f\"{name:20s}: {avg_time*1000:.4f} ms\")\n",
    "    \n",
    "    # verificacion\n",
    "    print(\"\\n3. VERIFICACION: Dan el mismo resultado?\")\n",
    "    print(\"-\"*50)\n",
    "    names = list(results.keys())\n",
    "    y_ref = results[names[0]][0]\n",
    "    scores_ref = results[names[0]][1]\n",
    "    \n",
    "    all_correct = True\n",
    "    for name in names[1:]:\n",
    "        y_test = results[name][0]\n",
    "        scores_test = results[name][1]\n",
    "        \n",
    "        pred_match = np.all(y_test == y_ref)\n",
    "        scores_close = np.allclose(scores_test, scores_ref, rtol=1e-5, atol=1e-8)\n",
    "        \n",
    "        status = \"✓\" if (pred_match and scores_close) else \"✗\"\n",
    "        print(f\"{status} {name:20s} ≡ {names[0]}: Pred={pred_match}, Scores={scores_close}\")\n",
    "        all_correct = all_correct and pred_match and scores_close\n",
    "    \n",
    "    # speedup\n",
    "    print(\"\\n4. SPEEDUP\")\n",
    "    print(\"-\"*50)\n",
    "    baseline_predict = predict_times[names[0]]\n",
    "    for name in results.keys():\n",
    "        speedup = baseline_predict / predict_times[name]\n",
    "        print(f\"{name:20s}: {speedup:.2f}x\")\n",
    "    \n",
    "    # resultado\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"RESULTADO\")\n",
    "    print(\"=\"*50)\n",
    "    if all_correct:\n",
    "        print(\"✓ TensorizedChol paraleliza correctamente\")\n",
    "        speedup_achieved = baseline_predict / predict_times['TensorizedChol']\n",
    "        print(f\"✓ Speedup logrado: {speedup_achieved:.2f}x\")\n",
    "        print(f\"✓ Tiempo de predicción: {predict_times['TensorizedChol']*1000:.4f} ms\")\n",
    "        print(\"✓ Precisión: Idéntica a baseline\")\n",
    "    else:\n",
    "        print(\"✗ Error: Resultados no coinciden\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "\n",
    "# ejecucion\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    np.random.seed(42)  # generamos datos de prueba\n",
    "    n_samples = 5000\n",
    "    n_features = 50\n",
    "    K = 3\n",
    "    \n",
    "    print(\"\\n DATOS\")\n",
    "    print(f\"  - Muestras: {n_samples}\")\n",
    "    print(f\"  - Features: {n_features}\")\n",
    "    print(f\"  - Clases: {K}\")\n",
    "    \n",
    "    X = np.random.randn(n_samples, n_features)\n",
    "    y = np.random.randint(0, K, n_samples)\n",
    "    \n",
    "    # ejecutamos benchmark\n",
    "    benchmark_qda_implementations(X, y, n_runs=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enunciado N°14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "amia-workspace-vazquez",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
